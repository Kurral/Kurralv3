# How to Perform Replay

This guide explains how to replay artifacts generated by your agents.

## Quick Start

### Method 1: Using the replay script (Recommended)

```bash
# From the ABTester directory
python ABTester/replay.py <artifact_path>
```

**Example:**
```bash
cd d:\ABTester
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral
```

### Method 2: Using Python module

```bash
# From the ABTester directory
python -m ABTester.cli.replay_cmd <artifact_path>
```

**Example:**
```bash
cd d:\ABTester
python -m ABTester.cli.replay_cmd agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral
```

## Command Options

### Replay by artifact file path
```bash
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral
```

### Replay by run_id
```bash
python ABTester/replay.py --run-id local_run_graph_with_kurral_1763472818 --storage-path agent1/artifacts
```

### Replay latest artifact
```bash
python ABTester/replay.py --latest --storage-path agent1/artifacts
```

### Replay with different LLM (B replay)
```bash
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral --llm-client openai
```

### Replay with different model
```bash
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral --current-model gpt-4-turbo
```

### Show diff between original and replay
```bash
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral --diff
```

### Verbose output
```bash
python ABTester/replay.py agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral --verbose
```

## Understanding Replay Types

### A Replay (Deterministic)
- **When**: Determinism score < 0.8 AND no changes detected
- **What happens**: Returns outputs directly from the artifact (no LLM execution)
- **Use case**: When everything is identical to the original run

### B Replay (Non-deterministic)
- **When**: Determinism score >= 0.8 OR tools/LLM changed
- **What happens**: Re-executes the LLM but uses cached tool calls
- **Use case**: When you want to test a different LLM with the same tool inputs

## Finding Artifacts

Artifacts are stored in each agent's `artifacts/` folder:

- `agent1/artifacts/` - Artifacts from agent1
- `agent2/artifacts/` - Artifacts from agent2 (if you have one)
- etc.

Each artifact file is named with its `kurral_id` (UUID):
```
agent1/artifacts/961061f9-8029-4949-a07e-11a028d75cc4.kurral
```

An `index.json` file in each artifacts folder contains metadata about all artifacts.

## Example Output

```
Replaying artifact: 961061f9-8029-4949-a07e-11a028d75cc4
Original run: local_run_graph_with_kurral_1763472818

Replay Type: A
Determinism Score: 0.82 (threshold: 0.80)
No changes detected

+----------------+
| Replay Outputs |
+----------------+
{
  "question": "What is the capital of france",
  "output": "...",
  ...
}

[SUCCESS] Replay completed in 0ms
Replay type: A
Cache hits: 2
Cache misses: 0
Hash match: [SUCCESS] (118c0161 -> 118c0161)
Structural match: [SUCCESS]
[SUCCESS] Outputs match original
```

## Troubleshooting

### Error: "Must provide artifact, --run-id, or --latest"
Make sure you're providing one of:
- Artifact file path as first argument
- `--run-id <run_id>` option
- `--latest` flag

### Error: "OPENAI_API_KEY not set (required for B replay)"
For B replays, you need to set the appropriate API key:
```bash
# Windows PowerShell
$env:OPENAI_API_KEY="your-key-here"

# Or create/update .env file in agent folder
```

### Artifact not found
- Check that the artifact path is correct
- Make sure you're running from the `ABTester` directory
- Use `--storage-path` to specify a different artifacts directory

