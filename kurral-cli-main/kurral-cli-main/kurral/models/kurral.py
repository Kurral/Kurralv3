"""
Core Pydantic models for .kurral artifact schema
"""

import json
import hashlib
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Optional, List
from uuid import UUID, uuid4

from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator


class ReplayLevel(str, Enum):
    """
    Determinism classification levels for replay fidelity
    
    A: Byte-for-byte identical output (frozen model + frozen tools + frozen clock)
    B: Structurally equivalent output (identical tool I/O + sampler, model may differ)
    C: Task-level equivalence only (use recorded tool outputs, assert task metrics)
    """

    A = "A"  # Byte-for-byte replay (score >= 0.90)
    B = "B"  # Structural equivalence (0.50 <= score < 0.90)
    C = "C"  # Task-level equivalence (score < 0.50)


class LLMParameters(BaseModel):
    """Detailed LLM parameters for determinism tracking (null values excluded)"""
    
    temperature: float = Field(..., ge=0.0, le=2.0)
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0)
    top_k: Optional[int] = Field(None, ge=0)
    max_tokens: Optional[int] = Field(None, ge=1)
    frequency_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    presence_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)
    seed: Optional[int] = Field(None, description="Random seed for determinism")


class TokenUsage(BaseModel):
    """Comprehensive token usage and cost metrics"""
    
    # Provider counters (from API response)
    prompt_tokens: int = Field(default=0, ge=0, description="Input tokens sent to LLM")
    completion_tokens: int = Field(default=0, ge=0, description="Output tokens generated by LLM")
    total_tokens: int = Field(default=0, ge=0, description="Total tokens (prompt + completion)")
    
    # Caching metrics (if provider supports it)
    cached_tokens: Optional[int] = Field(None, ge=0, description="Tokens served from cache")
    cache_creation_tokens: Optional[int] = Field(None, ge=0, description="Tokens used to populate cache")
    cache_read_tokens: Optional[int] = Field(None, ge=0, description="Tokens read from cache")
    
    # Cache efficiency
    cache_hit_rate: Optional[float] = Field(None, ge=0.0, le=1.0, description="Ratio of cached to total tokens")
    
    # Cost calculation
    estimated_cost_usd: Optional[float] = Field(None, ge=0.0, description="Estimated cost in USD")
    
    # Additional metrics
    reasoning_tokens: Optional[int] = Field(None, ge=0, description="Tokens used for reasoning (if applicable)")
    audio_tokens: Optional[int] = Field(None, ge=0, description="Audio tokens (if applicable)")


class ModelConfig(BaseModel):
    """Enhanced LLM model configuration with detailed parameters"""

    model_name: str = Field(..., description="Model identifier (e.g., gpt-4-0613)")
    model_version: Optional[str] = Field(None, description="Specific version hash if available")
    provider: str = Field(..., description="Provider (openai, anthropic, etc.)")
    parameters: LLMParameters = Field(..., description="Detailed model parameters")
    stop_sequences: Optional[list[str]] = None
    additional_params: Optional[dict[str, Any]] = Field(default=None, description="Additional provider-specific params")

    def cache_key(self) -> str:
        """Generate cache key from model config"""
        data = self.model_dump(exclude={"additional_params"})
        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()


class ResolvedPrompt(BaseModel):
    """Enhanced prompt template with hashing for replay fidelity"""

    template: str = Field(..., description="Original prompt template")
    template_id: Optional[str] = Field(None, description="Template identifier if versioned")
    template_hash: Optional[str] = Field(None, description="SHA256 hash of template")
    variables: dict[str, Any] = Field(
        default_factory=dict, description="Template variables used"
    )
    variables_hash: Optional[str] = Field(None, description="SHA256 hash of variables")
    final_text: str = Field(..., description="Final resolved prompt text")
    final_text_hash: Optional[str] = Field(None, description="SHA256 hash of final text")
    system_prompt: Optional[str] = Field(None, description="System prompt if separate")
    messages: Optional[list[dict[str, str]]] = Field(
        None, description="Chat messages if applicable"
    )

    @model_validator(mode="after")
    def compute_hashes(self) -> "ResolvedPrompt":
        """Auto-compute hashes if not provided"""
        if self.template_hash is None and self.template:
            self.template_hash = hashlib.sha256(self.template.encode()).hexdigest()
        
        if self.variables_hash is None and self.variables:
            try:
                # Try to serialize variables, filtering out non-serializable items
                serializable_vars = self._make_serializable(self.variables)
                vars_str = json.dumps(serializable_vars, sort_keys=True)
                self.variables_hash = hashlib.sha256(vars_str.encode()).hexdigest()
            except (TypeError, ValueError):
                # If still can't serialize, hash the string representation
                vars_str = str(self.variables)
                self.variables_hash = hashlib.sha256(vars_str.encode()).hexdigest()
        
        if self.final_text_hash is None and self.final_text:
            self.final_text_hash = hashlib.sha256(self.final_text.encode()).hexdigest()
        return self
    
    @staticmethod
    def _make_serializable(obj: Any, max_depth: int = 3, current_depth: int = 0) -> Any:
        """Make an object JSON serializable by filtering out problematic types"""
        if current_depth > max_depth:
            return "<max_depth>"
        
        if obj is None or isinstance(obj, (bool, int, float, str)):
            return obj
        
        if isinstance(obj, (list, tuple)):
            return [ResolvedPrompt._make_serializable(item, max_depth, current_depth + 1) for item in obj]
        
        if isinstance(obj, dict):
            result = {}
            for k, v in obj.items():
                # Skip callbacks and other non-serializable objects
                if k == "callbacks" or callable(v):
                    continue
                try:
                    result[str(k)] = ResolvedPrompt._make_serializable(v, max_depth, current_depth + 1)
                except:
                    result[str(k)] = f"<{type(v).__name__}>"
            return result
        
        # Try JSON serialization test
        try:
            json.dumps(obj)
            return obj
        except (TypeError, ValueError):
            return f"<{type(obj).__name__}>"

    def cache_key(self) -> str:
        """Generate cache key from prompt"""
        return self.final_text_hash or hashlib.sha256(self.final_text.encode()).hexdigest()


class EffectType(str, Enum):
    """Types of side effects a tool can have"""
    HTTP = "http"
    DB_WRITE = "db_write"
    EMAIL = "email"
    FS = "fs"
    MCP = "mcp"
    OTHER = "other"


class ToolCallStatus(str, Enum):
    """Status of tool call execution"""
    OK = "ok"
    ERROR = "error"
    TIMEOUT = "timeout"


class ToolCall(BaseModel):
    """Enhanced tool call record with stubbing and effect tracking"""

    # Identity and hierarchy
    id: str = Field(default_factory=lambda: str(uuid4()), description="Unique tool call ID")
    parent_id: Optional[str] = Field(None, description="Parent tool call ID for nested/chained calls")
    tool_name: str = Field(..., description="Name of the tool/function called")
    namespace: Optional[str] = Field(None, description="Tool namespace (e.g., mcp server)")
    agent_id: Optional[str] = Field(None, description="Agent that made the call")
    
    # Timing - explicit start and end for concurrency analysis
    start_time: datetime = Field(default_factory=datetime.utcnow, description="When execution started")
    end_time: Optional[datetime] = Field(None, description="When execution completed")
    latency_ms: Optional[int] = Field(None, description="Execution time in milliseconds")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Legacy: use start_time")
    
    # Inputs/Outputs with hashing
    input: dict[str, Any] = Field(default_factory=dict, description="Input parameters")
    input_hash: Optional[str] = Field(None, description="SHA256 hash of input")
    output: dict[str, Any] = Field(default_factory=dict, description="Tool output/response")
    output_hash: Optional[str] = Field(None, description="SHA256 hash of output")
    summary: Optional[str] = Field(None, description="Human-readable summary of call")
    
    # Classification and tracking
    type: str = Field(default="tool", description="Operation type: tool, llm, agent, retrieval, etc.")
    effect_type: EffectType = Field(default=EffectType.OTHER, description="Type of side effect")
    status: ToolCallStatus = Field(default=ToolCallStatus.OK, description="Execution status")
    
    # Error handling
    error_flag: bool = Field(default=False, description="Whether call resulted in error")
    error_text: Optional[str] = Field(None, description="Error message if failed")
    error_type: Optional[str] = Field(None, description="Error type/class for categorization")
    error_stack: Optional[str] = Field(None, description="Full stack trace for debugging")
    
    # Caching and replay
    cache_key: str = Field(..., description="Content-addressed cache key")
    stubbed_in_replay: bool = Field(default=False, description="Was this stubbed in replay?")
    
    # Extensibility
    metadata: dict[str, Any] = Field(
        default_factory=dict, 
        description="Extensible metadata: cost, tokens, provider info, custom attributes"
    )
    
    # Legacy support (for backward compatibility)
    inputs: Optional[dict[str, Any]] = Field(None, description="Legacy: use 'input' instead")
    outputs: Optional[dict[str, Any]] = Field(None, description="Legacy: use 'output' instead")
    error: Optional[str] = Field(None, description="Legacy: use 'error_text' instead")
    duration_ms: Optional[int] = Field(None, description="Legacy: use 'latency_ms' instead")

    @staticmethod
    def generate_cache_key(tool_name: str, inputs: dict[str, Any]) -> str:
        """Generate deterministic cache key from tool call"""
        data = {"tool": tool_name, "inputs": inputs}
        content = json.dumps(data, sort_keys=True)
        return f"sha256:{hashlib.sha256(content.encode()).hexdigest()}"

    @model_validator(mode="before")
    @classmethod
    def ensure_cache_key_and_hashes(cls, values: dict[str, Any]) -> dict[str, Any]:
        """Auto-generate cache key, hashes, and timing if not provided"""
        # Handle legacy field names
        if "inputs" in values and "input" not in values:
            values["input"] = values["inputs"]
        if "outputs" in values and "output" not in values:
            values["output"] = values["outputs"]
        if "error" in values and values["error"] and "error_text" not in values:
            values["error_text"] = values["error"]
            values["error_flag"] = True
        if "duration_ms" in values and "latency_ms" not in values:
            values["latency_ms"] = values["duration_ms"]
        
        # Calculate end_time from start_time + latency if not provided
        if "end_time" not in values and "start_time" in values and "latency_ms" in values:
            from datetime import timedelta
            start = values["start_time"]
            if isinstance(start, datetime) and values["latency_ms"]:
                values["end_time"] = start + timedelta(milliseconds=values["latency_ms"])
        
        # Calculate latency_ms from start_time and end_time if not provided
        if "latency_ms" not in values and "start_time" in values and "end_time" in values:
            start = values["start_time"]
            end = values["end_time"]
            if isinstance(start, datetime) and isinstance(end, datetime):
                values["latency_ms"] = int((end - start).total_seconds() * 1000)
            
        # Generate cache key
        if "cache_key" not in values or not values["cache_key"]:
            tool_name = values.get("tool_name", "unknown")
            input_data = values.get("input", values.get("inputs", {}))
            values["cache_key"] = cls.generate_cache_key(tool_name, input_data)
        
        # Generate hashes
        if "input_hash" not in values and values.get("input"):
            input_str = json.dumps(values["input"], sort_keys=True)
            values["input_hash"] = hashlib.sha256(input_str.encode()).hexdigest()
        
        if "output_hash" not in values and values.get("output"):
            output_str = json.dumps(values["output"], sort_keys=True)
            values["output_hash"] = hashlib.sha256(output_str.encode()).hexdigest()
        
        return values


class GraphVersion(BaseModel):
    """Graph and tool versioning for determinism tracking"""
    
    model_config = ConfigDict(exclude_none=True)
    
    graph_hash: Optional[str] = Field(None, description="SHA256 hash of compiled graph structure")
    graph_checksum: Optional[str] = Field(None, description="Checksum including node/edge configs")
    tool_schemas_hash: Optional[str] = Field(None, description="Combined hash of all tool schemas")
    tools: Optional[list[dict[str, Any]]] = Field(None, description="Individual tool definitions with hashes")
    policy_pack_hash: Optional[str] = Field(None, description="Hash of active policy pack")
    policy_version: Optional[str] = Field(None, description="Policy pack version string")
    preprocessors_hash: Optional[str] = Field(None, description="Hash of all preprocessors")
    postprocessors_hash: Optional[str] = Field(None, description="Hash of all postprocessors")


class TimeEnvironment(BaseModel):
    """Extended time and environment snapshot with feature flags"""

    timestamp: datetime = Field(..., description="Execution timestamp (ISO 8601)")
    timezone: str = Field(default="UTC")
    wall_clock_time: str = Field(..., description="Human-readable time")
    locale: Optional[str] = Field(None, description="Locale setting (e.g., en_US)")
    environment_vars: dict[str, str] = Field(
        default_factory=dict, description="Relevant env vars (sanitized)"
    )
    feature_flags: dict[str, Any] = Field(
        default_factory=dict, description="Feature flags active during execution"
    )
    clock_freeze: Optional[bool] = Field(None, description="Was time frozen for testing? (omitted if false/null)")


class DeterminismReport(BaseModel):
    """Determinism analysis and scoring"""

    overall_score: float = Field(..., ge=0.0, le=1.0, description="Overall score (0.0-1.0)")
    breakdown: dict[str, float] = Field(..., description="Score breakdown by factor")
    missing_fields: list[str] = Field(default_factory=list)
    warnings: list[str] = Field(default_factory=list)

    @field_validator("breakdown")
    @classmethod
    def validate_breakdown(cls, v: dict[str, float]) -> dict[str, float]:
        """Ensure breakdown scores are in valid range"""
        for key, score in v.items():
            if not 0.0 <= score <= 1.0:
                raise ValueError(f"Score for {key} must be between 0.0 and 1.0")
        return v


class CachePrimerSource(str, Enum):
    """Source of cache primer data"""
    PRODUCTION = "production"
    KURRAL = ".kurral"
    MANUAL = "manual"


class HashChain(BaseModel):
    """Hash chain for replay integrity verification"""
    
    root: str = Field(..., description="Root hash of the chain")
    spans: list[str] = Field(default_factory=list, description="Hash spans for verification")


class CachePrimer(BaseModel):
    """Cache primer information for replay"""
    
    source: CachePrimerSource = Field(..., description="Source of cache data")
    ref_id: Optional[str] = Field(None, description="Reference ID to source")


class ReplayEvidence(BaseModel):
    """Evidence and metadata from replay execution"""
    
    replay_id: str = Field(..., description="Unique replay execution ID")
    replay_start_at: datetime = Field(..., description="Replay start timestamp")
    replay_end_at: datetime = Field(..., description="Replay end timestamp")
    cache_primer: CachePrimer = Field(..., description="Cache primer information")
    effects_stubbed_count: int = Field(default=0, description="Number of effects stubbed")
    effects_executed_count: int = Field(default=0, description="Number of effects executed")
    hash_chain: HashChain = Field(..., description="Hash chain for integrity")


class Governance(BaseModel):
    """Governance and policy tracking"""
    
    policy_version: Optional[str] = Field(None, description="Policy version applied")
    policy_bundle_hash: Optional[str] = Field(None, description="SHA256 of policy bundle")
    eval_version: Optional[str] = Field(None, description="Evaluation framework version")
    eval_defs_hash: Optional[str] = Field(None, description="SHA256 of eval definitions")


class DataSnapshot(BaseModel):
    """Data snapshot reference for reproducibility"""
    
    name: str = Field(..., description="Snapshot name/identifier")
    uri: str = Field(..., description="URI to snapshot data")
    hash: str = Field(..., description="SHA256 hash of snapshot")
    version: Optional[str] = Field(None, description="Snapshot version")


class MCPServer(BaseModel):
    """MCP Server metadata"""
    
    server_id: str = Field(..., description="Server identifier")
    version: Optional[str] = Field(None, description="Server version")
    namespace_hint: Optional[str] = Field(None, description="Suggested namespace")
    tool_count: int = Field(default=0, description="Number of tools provided")
    tested_models: list[str] = Field(default_factory=list, description="Models tested with")
    notes: Optional[str] = Field(None, description="Additional notes")


class MCPTool(BaseModel):
    """MCP Tool metadata"""
    
    fq_tool: str = Field(..., description="Fully qualified tool name")
    raw_name: str = Field(..., description="Raw tool name")
    server_id: str = Field(..., description="Server providing this tool")
    schema_depth_max: int = Field(default=0, description="Maximum schema depth")
    param_count: int = Field(default=0, description="Number of parameters")
    requires_auth: bool = Field(default=False, description="Requires authentication")


class MCPCallStats(BaseModel):
    """Statistics for MCP tool calls"""
    
    fq_tool: str = Field(..., description="Fully qualified tool name")
    avg_tokens: Optional[int] = Field(None, description="Average tokens used")
    max_tokens: Optional[int] = Field(None, description="Maximum tokens used")
    latency_ms: Optional[int] = Field(None, description="Average latency")
    error_flag: bool = Field(default=False, description="Had errors")
    error_text_present: bool = Field(default=False, description="Error text available")
    resource_link_used: bool = Field(default=False, description="Used resource links")


class MCPCatalogStats(BaseModel):
    """Statistics about MCP tool catalog"""
    
    tool_count_enabled: int = Field(default=0, description="Number of enabled tools")
    name_collisions: list[str] = Field(default_factory=list, description="Tool name collisions")
    semantically_similar: list[list[str]] = Field(
        default_factory=list, description="Semantically similar tool groups"
    )
    schema_depth_histogram: dict[str, int] = Field(
        default_factory=dict, description="Distribution of schema depths"
    )


class MCPContext(BaseModel):
    """MCP-specific context and statistics"""
    
    servers: list[MCPServer] = Field(default_factory=list, description="MCP servers used")
    tools: list[MCPTool] = Field(default_factory=list, description="Available tools")
    calls: list[MCPCallStats] = Field(default_factory=list, description="Call statistics")
    catalog_stats: Optional[MCPCatalogStats] = Field(None, description="Catalog statistics")


class Extensions(BaseModel):
    """Extension fields for additional context"""
    
    mcp_context: Optional[MCPContext] = Field(None, description="MCP-specific context")
    namespace_map: dict[str, str] = Field(
        default_factory=dict, description="Mapping from raw names to fully qualified names"
    )


class KurralArtifact(BaseModel):
    """
    Complete .kurral artifact - the core data structure
    """

    # Identity
    kurral_id: UUID = Field(default_factory=uuid4, description="Unique artifact ID")
    run_id: str = Field(..., description="Source trace/run ID (e.g., from LangSmith)")
    tenant_id: str = Field(..., description="Tenant/organization identifier")
    semantic_buckets: list[str] = Field(
        default_factory=list, description="Business logic categories"
    )
    environment: str = Field(default="production", description="Environment (prod/staging/dev)")

    # Metadata
    schema_version: str = Field(default="1.0.0", description="Artifact schema version")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    created_by: Optional[str] = Field(None, description="User/service that created artifact")

    # Determinism
    deterministic: bool = Field(..., description="Overall determinism flag")
    replay_level: ReplayLevel = Field(..., description="Replay reliability level (A/B/C)")
    determinism_report: DeterminismReport = Field(..., description="Detailed scoring")

    # Execution Data
    inputs: dict[str, Any] = Field(..., description="Function/agent inputs")
    outputs: dict[str, Any] = Field(..., description="Function/agent outputs")
    error: Optional[str] = Field(None, description="Error message if execution failed")

    # LLM Configuration
    llm_config: ModelConfig = Field(..., description="Model configuration")
    resolved_prompt: ResolvedPrompt = Field(..., description="Prompt with variables")

    # Graph & Tool Versioning
    graph_version: Optional[GraphVersion] = Field(None, description="Graph and tool schema hashes")

    # Tool Calls
    tool_calls: list[ToolCall] = Field(default_factory=list, description="All tool invocations")

    # Time & Environment
    time_env: TimeEnvironment = Field(..., description="Time and environment snapshot")

    # Metrics
    duration_ms: int = Field(..., ge=0, description="Total execution time")
    cost_usd: Optional[float] = Field(None, ge=0.0, description="Estimated cost in USD")
    token_usage: TokenUsage = Field(
        default_factory=TokenUsage, description="Comprehensive token usage and cost metrics"
    )

    # Storage
    object_storage_uri: Optional[str] = Field(
        None, description="R2/storage URI if stored externally"
    )
    tags: dict[str, str] = Field(default_factory=dict, description="Custom tags")
    
    # New Enhanced Fields
    replay_evidence: Optional[ReplayEvidence] = Field(
        None, description="Evidence from replay execution"
    )
    governance: Optional[Governance] = Field(
        None, description="Governance and policy information"
    )
    data_snapshots: list[DataSnapshot] = Field(
        default_factory=list, description="Data snapshots for reproducibility"
    )
    extensions: Optional[Extensions] = Field(
        None, description="Extension fields including MCP context"
    )

    @model_validator(mode="after")
    def validate_replay_level(self) -> "KurralArtifact":
        """Ensure replay level matches determinism score"""
        score = self.determinism_report.overall_score

        expected_level = ReplayLevel.C
        if score >= 0.90:
            expected_level = ReplayLevel.A
        elif score >= 0.50:
            expected_level = ReplayLevel.B

        if self.replay_level != expected_level:
            raise ValueError(
                f"Replay level {self.replay_level} doesn't match score {score} "
                f"(expected {expected_level})"
            )

        # Set deterministic flag based on score
        if score >= 0.90:
            object.__setattr__(self, "deterministic", True)
        else:
            object.__setattr__(self, "deterministic", False)

        return self

    def to_json(self, pretty: bool = False) -> str:
        """Serialize to JSON string"""
        indent = 2 if pretty else None
        return self.model_dump_json(indent=indent, exclude_none=True)

    def save(self, filepath: str | Path) -> None:
        """Save artifact to .kurral file"""
        path = Path(filepath)
        path.parent.mkdir(parents=True, exist_ok=True)

        with open(path, "w") as f:
            f.write(self.to_json(pretty=True))

    @classmethod
    def load(cls, filepath: str | Path) -> "KurralArtifact":
        """Load artifact from .kurral file"""
        with open(filepath, "r") as f:
            data = json.load(f)
        return cls.model_validate(data)

    @classmethod
    def from_json(cls, json_str: str) -> "KurralArtifact":
        """Deserialize from JSON string"""
        return cls.model_validate_json(json_str)

    def cache_key(self) -> str:
        """Generate cache key for this artifact"""
        data = {
            "model": self.llm_config.cache_key(),
            "prompt": self.resolved_prompt.cache_key(),
            "inputs": self.inputs,
        }
        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()


class ReplayOverrides(BaseModel):
    """Optional overrides for replay execution"""

    prompt: Optional[str] = Field(None, description="Override prompt text")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0)
    max_tokens: Optional[int] = Field(None, ge=1)
    model_name: Optional[str] = None
    inputs: Optional[dict[str, Any]] = None


class ReplayResult(BaseModel):
    """Result of a replay execution"""

    kurral_id: UUID = Field(..., description="Original artifact ID")
    replay_timestamp: datetime = Field(default_factory=datetime.utcnow)
    outputs: dict[str, Any] = Field(..., description="Replay outputs")
    match: bool = Field(..., description="Whether outputs match original")
    diff: Optional[dict[str, Any]] = Field(None, description="Differences if any")
    tool_calls: list[ToolCall] = Field(default_factory=list)
    error: Optional[str] = None
    duration_ms: int = Field(..., ge=0)
    cache_hits: int = Field(default=0, ge=0)
    cache_misses: int = Field(default=0, ge=0)
    stream: Optional[dict[str, Any]] = Field(
        None,
        description="Reconstructed output stream with items/full_text/stream_map",
    )
    graph_version: Optional["GraphVersion"] = Field(
        None, description="Pinned graph metadata used during replay"
    )
    llm_state: Optional["ReplayLLMState"] = Field(
        None, description="Hydrated LLM sampling state used during replay"
    )
    validation: Optional["ReplayValidation"] = Field(
        None, description="Replay validation metrics (hash + structural comparisons)"
    )
    replay_metadata: Optional["ReplayMetadata"] = Field(
        None, description="Metadata describing the replay execution and assertions"
    )


class ReplayLLMState(BaseModel):
    """Hydrated sampling parameters for replay"""

    model_name: str
    provider: str
    model_version: Optional[str] = None
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    top_k: Optional[int] = None
    max_tokens: Optional[int] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    seed: Optional[int] = None


class ReplayValidation(BaseModel):
    """Validation information for comparing original and replay outputs"""

    original_hash: str
    replay_hash: str
    hash_match: bool
    structural_match: bool
    diff: Optional[dict[str, Any]] = None


class AssertionResult(BaseModel):
    """Represents the outcome of a replay assertion"""

    name: str
    passed: bool
    details: Optional[dict[str, Any]] = None


class ReplayMetadata(BaseModel):
    """Metadata describing a replay execution"""

    replay_id: str
    record_ref: str
    replay_level: ReplayLevel
    assertion_results: List[AssertionResult] = Field(default_factory=list)


class BacktestRequest(BaseModel):
    """Request for backtest execution"""

    baseline_artifacts: list[UUID] = Field(..., description="Baseline artifact IDs")
    candidate_config: dict[str, Any] = Field(..., description="New agent configuration")
    threshold: float = Field(default=0.90, ge=0.0, le=1.0)
    sample_strategy: str = Field(default="adaptive", description="Sampling strategy")
    max_replays: int = Field(default=100, ge=1)


class BacktestResult(BaseModel):
    """Result of backtest execution"""

    backtest_id: UUID = Field(default_factory=uuid4)
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    baseline_count: int = Field(..., ge=0)
    replays_executed: int = Field(..., ge=0)
    ars_score: float = Field(..., ge=0.0, le=1.0, description="Agent Regression Score")
    passed: bool = Field(..., description="Whether backtest passed threshold")
    threshold: float = Field(..., ge=0.0, le=1.0)
    breakdown: dict[str, Any] = Field(default_factory=dict)
    failures: list[dict[str, Any]] = Field(default_factory=list)
    summary: str = Field(..., description="Human-readable summary")

